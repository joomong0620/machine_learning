{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Naive Bayes\n",
    " 나이브베이즈 분류문제를 해결하기 위한 확률 기반 머신러닝 알고리즘이다.\n",
    "\n"
   ],
   "id": "cdf1e4ddd72f031"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Naive Bayes의 기본 개념**\n",
    "\n",
    "\n",
    "나이브 베이즈는 입력 `X`가 주어졌을 때, 각 클래스 `Class`가 될 **사후확률(Posterior Probability)**\n",
    "`P(Class | X)`를 계산하고, 그 값이 가장 큰 클래스를 선택하는 알고리즘이다.\n",
    "\n",
    "\n",
    "이때 사용하는 핵심 공식이 바로 **베이즈 정리(Bayes' theorem)** 이다.\n",
    "\n",
    "\n",
    "**베이즈 정리(Bayes' Theorem)**\n",
    "\n",
    "\n",
    "   나이브 베이즈는 베이즈 정리를 이용해 사후확률을 계산한다.\n",
    "\n",
    "\n",
    "   $$\n",
    "   P(Class \\mid X) = \\frac{P(X \\mid Class) \\cdot P(Class)}{P(X)}\n",
    "   $$\n",
    "\n",
    "\n",
    "   - `P(Class | X)` : 입력 `X`가 주어졌을 때, 클래스 `Class`일 확률 (사후확률)\n",
    "   - `P(X | Class)` : 특정 클래스일 때 입력 `X`가 관측될 확률 (우도)\n",
    "   - `P(Class)` : 그 클래스 자체의 확률 (사전확률)\n",
    "   - `P(X)` : 전체 데이터에서 `X`가 나올 확률 (정규화 상수)\n",
    "\n",
    "\n",
    "   분류 문제에서 클래스들끼리 비교할 때는 분모 `P(X)`가 공통이므로\n",
    "   일반적으로 다음 형태로만 비교해도 된다.\n",
    "\n",
    "\n",
    "   $$\n",
    "   P(Class \\mid X) \\propto P(X \\mid Class) \\cdot P(Class)\n",
    "   $$\n",
    "---\n",
    "\n",
    "\n",
    "**Naive Bayes의 핵심 요소**\n",
    "\n",
    "\n",
    "1. **사전확률(Prior Probability)**\n",
    "   - 데이터를 보기 “이전”에 각 클래스가 등장할 확률\n",
    "   - 예: 전체 메일 중 스팸 메일 비율\n",
    "\n",
    "\n",
    "2. **우도(Likelihood)**\n",
    "   - “특정 클래스라고 가정했을 때, 현재 입력 X가 나올 확률”\n",
    "   - 예: 스팸 메일일 때 단어 `광고`가 등장할 확률\n",
    "\n",
    "\n",
    "3. **사후확률(Posterior Probability)**\n",
    "   - 입력 X가 주어졌을 때 실제로 어떤 클래스인지에 대한 확률\n",
    "   - 나이브 베이즈는 이 값을 최대화하는 클래스를 예측값으로 선택\n",
    "\n",
    "\n",
    "4. **독립 가정(Conditional Independence)**\n",
    "   - 모든 특성들이 서로 독립이라고 가정\n",
    "   - 이 가정 덕분에 계산이 단순해지고 속도가 매우 빨라진다.\n",
    "   - 입력 벡터 `X = (x₁, x₂, …, xₙ)` 에 대해 다음과 같이 근사한다.\n",
    "\n",
    "\n",
    "   $$\n",
    "   P(X \\mid Class)\n",
    "   = P(x_1 \\mid Class) \\cdot P(x_2 \\mid Class) \\cdot \\dots \\cdot P(x_n \\mid Class)\n",
    "   $$\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Naive Bayes의 종류**\n",
    "\n",
    "\n",
    "- **Gaussian Naive Bayes**\n",
    "  - 연속형(숫자) 특성\n",
    "  - 각 특성이 정규분포를 따른다고 가정\n",
    "  - 예: 키, 몸무게, 길이 등 연속형 데이터 분류\n",
    "\n",
    "\n",
    "- **Multinomial Naive Bayes**\n",
    "  - 단어 개수(빈도) 기반 데이터\n",
    "  - Bag-of-Words, TF-IDF 벡터와 함께 사용\n",
    "  - 예: 뉴스 기사 분류, 문서 카테고리 분류\n",
    "\n",
    "\n",
    "- **Bernoulli Naive Bayes**\n",
    "  - 0/1 (True/False)로 표현되는 특성\n",
    "  - 단어가 문서에 “존재한다 / 존재하지 않는다” 여부 기반\n",
    "  - 예: 스팸메일, 특정 키워드 여부가 중요한 경우\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**라플라스 스무딩(Laplace Smoothing)**\n",
    "\n",
    "\n",
    "텍스트 분류에서 특정 클래스에서 어떤 단어가 한 번도 등장하지 않으면,\n",
    "그 단어의 조건부 확률이 0이 된다.\n",
    "\n",
    "\n",
    "이때 여러 단어 확률을 곱하면 `전체 확률이 0`이 되는 문제가 발생한다.\n",
    "이를 방지하기 위해 **스무딩(smoothing)** 을 사용한다.\n",
    "\n",
    "\n",
    "라플라스 스무딩 공식은 다음과 같다.\n",
    "\n",
    "\n",
    "$$\n",
    "P(word \\mid class)\n",
    "= \\frac{count + \\alpha}{total + \\alpha \\cdot V}\n",
    "$$\n",
    "\n",
    "\n",
    "- `count` : 해당 클래스에서 그 단어가 등장한 횟수\n",
    "- `total` : 해당 클래스에서 등장한 전체 단어 수\n",
    "- `V` : 전체 단어(단어 사전)의 크기\n",
    "- `α` : 스무딩 계수(일반적으로 1 사용)\n",
    "\n",
    "\n",
    "스무딩을 적용하면 한 번도 등장하지 않은 단어라도\n",
    "작은 확률을 부여하여 전체 확률이 0으로 붕괴되는 것을 막을 수 있다.\n",
    "\n",
    "\n"
   ],
   "id": "375cb4efc41acf2b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Gaussian Naive Bayes\n",
   "id": "56104f4fdd41ae0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T02:47:21.765160Z",
     "start_time": "2025-12-17T02:47:21.593099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ],
   "id": "6fd87a0c7690fab1",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T03:13:43.565279Z",
     "start_time": "2025-12-17T03:13:43.534036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "#1. 데이터 로드\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "#2. 학습/평가셋 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#3. 스케일링\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "#4. 모델 생성/학습\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "#5. 예측\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# 6.평가\n",
    "print('정확도(accuracy):', accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "confusion_matrix(y_test, y_pred)"
   ],
   "id": "d561b84abe75a7db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도(accuracy): 0.9666666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      setosa       1.00      1.00      1.00        10\n",
      "  versicolor       1.00      0.90      0.95        10\n",
      "   virginica       0.91      1.00      0.95        10\n",
      "\n",
      "    accuracy                           0.97        30\n",
      "   macro avg       0.97      0.97      0.97        30\n",
      "weighted avg       0.97      0.97      0.97        30\n",
      "\n",
      "[[10  0  0]\n",
      " [ 0  9  1]\n",
      " [ 0  0 10]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10,  0,  0],\n",
       "       [ 0,  9,  1],\n",
       "       [ 0,  0, 10]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
