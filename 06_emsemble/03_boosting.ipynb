{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 03.Boosting계열\n",
    "Boosting은 **여러 개의 약한 모델(Weak Learner 주로 얕은 결정 트리)** 을 **순차적으로 학습**시켜 **이전 모델의 오류를 보완**해나가는 **강한 예측기(Strong Predicter)** 를 사용하는 앙상블 기법이다.\n",
    "\n",
    "\n",
    "- 약한 모델(예: 작은 깊이의 결정 트리)을 순차적으로 학습\n",
    "- 이전 모델의 **오류(잔차)** 를 다음 모델이 보정(이전 모델이 틀린 샘플에 더 높은 가중치)\n",
    "- 각 모델의 예측을 **가중합**하여 최종 예측을 수행\n",
    "- 학습 방식: **순차적** / **오류 중심 개선**\n",
    "- 사용 모델: 주로 **깊이가 얕은 결정 트리**\n",
    "- 최적화 방식: **경사하강법(Gradient Descent)**\n",
    "\n",
    "\n",
    "| 장점 | 단점 |\n",
    "|------|------|\n",
    "| 과적합에 비교적 강함 | 트리를 **순차적으로 학습**하여 속도가 느림 |\n",
    "| 일반적으로 랜덤 포레스트보다 성능이 더 높음 | 병렬 처리 어려움 (순차 학습 구조) |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Boosting 알고리즘 발전 단계**\n",
    "- Boosting은 **오차 보정 방식의 순차적 학습 구조**로 앙상블을 구성\n",
    "- **Gradient Boosting**은 경사하강법으로 트리를 추가하며 오차를 줄임\n",
    "- **HistGradientBoosting**은 학습 속도와 메모리 효율성을 높인 개선된 버전 (노드분할 효율성을 위해 연속형 특성에 대한 binning 처리)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**손실 함수 및 학습 설정**\n",
    "\n",
    "\n",
    "| 항목 | 설명 |\n",
    "|------|------|\n",
    "| **분류** | 로지스틱 손실 함수(log loss) 사용 |\n",
    "| **회귀** | 평균제곱오차(MSE) 사용 |\n",
    "| **학습률 (learning_rate)** | 한 번에 추가되는 트리의 영향력을 조절 |\n",
    "| **subsample** | 훈련 세트 중 일부만 사용하면 확률적 경사하강법(SGD)처럼 작동 |\n",
    "\n",
    "\n",
    "> ⚠ `subsample < 1.0` → **일부 데이터로 학습하여 일반화 성능 향상 가능**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**주요 알고리즘 및 클래스**\n",
    "\n",
    "\n",
    "알고리즘 | 등장년도 | 핵심 특징\n",
    "--- | --- | ---\n",
    "AdaBoost | 1996 | 가중치를 이용해 오답 샘플을 강조\n",
    "Gradient Boosting (GBM) | 1999 | 잔차(오차)를 줄이도록 새 모델 학습\n",
    "XGBoost | 2014 | 정규화 + 병렬처리 최적화\n",
    "LightGBM | 2017 | 히스토그램 기반 + Leaf-wise 트리\n",
    "CatBoost | 2017 | 범주형 자동 처리 + 빠른 속도\n",
    "HistGradientBoosting (scikit-learn) | 2019~ | LightGBM과 유사한 히스토그램 기반 구현\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**히스토그램 기반 그레디언트 부스팅이란?**\n",
    "\n",
    "\n",
    "히스토그램 기반 그레디언트 부스팅은 기존 GBM의 **느린 학습 속도** 문제를 개선한 방식이다.\n",
    "\n",
    "\n",
    "- 연속 데이터를 **구간(binning)** 으로 나눠 계산량을 줄임\n",
    "- 속도는 빠르면서도 일반 GBM에 가까운 예측 성능 유지\n",
    "- 대용량 데이터 처리에 적합\n",
    "\n",
    "\n"
   ],
   "id": "981d66bac68f2b0a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T04:06:09.058733Z",
     "start_time": "2025-12-19T04:06:09.053812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from statistics import LinearRegression\n",
    "from turtledemo.sorting_animate import disable_keys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import VotingClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import root_mean_squared_error, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor"
   ],
   "id": "5e275e89caed172c",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 01. SimpleGradientBoostingClassifier  구현\n",
    "- 잔차를 학습하는 GradientBoosting 분류기를 직접 구현"
   ],
   "id": "47af9685583a3bf1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T04:06:09.121071Z",
     "start_time": "2025-12-19T04:06:09.098486Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "# 데이터 분리\n",
    "X = cancer.data\n",
    "y = cancer.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(X_train[:3])\n",
    "print(y_train[:3])"
   ],
   "id": "1579b6dce5160482",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (455,)\n",
      "(114, 30) (114,)\n",
      "[[1.032e+01 1.635e+01 6.531e+01 3.249e+02 9.434e-02 4.994e-02 1.012e-02\n",
      "  5.495e-03 1.885e-01 6.201e-02 2.104e-01 9.670e-01 1.356e+00 1.297e+01\n",
      "  7.086e-03 7.247e-03 1.012e-02 5.495e-03 1.560e-02 2.606e-03 1.125e+01\n",
      "  2.177e+01 7.112e+01 3.849e+02 1.285e-01 8.842e-02 4.384e-02 2.381e-02\n",
      "  2.681e-01 7.399e-02]\n",
      " [2.018e+01 1.954e+01 1.338e+02 1.250e+03 1.133e-01 1.489e-01 2.133e-01\n",
      "  1.259e-01 1.724e-01 6.053e-02 4.331e-01 1.001e+00 3.008e+00 5.249e+01\n",
      "  9.087e-03 2.715e-02 5.546e-02 1.910e-02 2.451e-02 4.005e-03 2.203e+01\n",
      "  2.507e+01 1.460e+02 1.479e+03 1.665e-01 2.942e-01 5.308e-01 2.173e-01\n",
      "  3.032e-01 8.075e-02]\n",
      " [1.066e+01 1.515e+01 6.749e+01 3.496e+02 8.792e-02 4.302e-02 0.000e+00\n",
      "  0.000e+00 1.928e-01 5.975e-02 3.309e-01 1.925e+00 2.155e+00 2.198e+01\n",
      "  8.713e-03 1.017e-02 0.000e+00 0.000e+00 3.265e-02 1.002e-03 1.154e+01\n",
      "  1.920e+01 7.320e+01 4.083e+02 1.076e-01 6.791e-02 0.000e+00 0.000e+00\n",
      "  2.710e-01 6.164e-02]]\n",
      "[1 0 1]\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T04:06:09.168258Z",
     "start_time": "2025-12-19T04:06:09.160832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 이진 분류\n",
    " # - sigmoid(z) : 숫자값을 인자로 받아 확률값 반환\n",
    " # - log_odds(p) : 확률값을 인자로 받아 log(숫자값 z)로 반환\n",
    "\n",
    " # 초기값\n",
    "y_pred  = np.mean(y_train)\n",
    "print('y_pred', y_pred) # 양성클래스 확률값\n",
    "\n",
    "# 로그오즈 계산 :  입력(p: 확률) -> 출력(z : 로그오즈)\n",
    "log_odds = lambda p: np.log(p/(1-p))\n",
    "z = log_odds(y_pred)\n",
    "print('z=', z)\n",
    "\n",
    "# 시그모이드 계산 : 입력 (z: 로그 오즈) -> 출력(p: 확률)\n",
    "sigmoid = lambda z:1 / (1+np.exp(-z))\n",
    "p = sigmoid(z)\n",
    "print('p=', p)"
   ],
   "id": "d486fd8acf22ef9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred 0.6263736263736264\n",
      "z= 0.5166907432183888\n",
      "p= 0.6263736263736264\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T04:06:10.158952Z",
     "start_time": "2025-12-19T04:06:09.242085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class SimpleGradientBoostingClassifier:\n",
    "    def __init__(self, n_estimators=100, lr=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.lr = lr\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        self.trees = [] # 약학습기를 관리할 리스트\n",
    "        self._inital_log_odds = 0 #로그오즈 초기값\n",
    "\n",
    "    def log_odds(self, p):\n",
    "        \"\"\"p(확률)을 입력 받아, z(로그오즈)를 반환하는 함수)\"\"\"\n",
    "        return np.log(p/(1-p))\n",
    "\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"z(로그오즈)을 입력 받아, p(확률)를 반환하는 함수)\"\"\"\n",
    "        return 1/(1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"모델 학습 함수\"\"\"\n",
    "        #부스팅은 기존의 모델이랑 새로운것을 더해야하기 때문에 초기로그오즈값이 필요하다.\n",
    "\n",
    "        # 초기 로그오즈값 설정 (y 평균값)\n",
    "        y_mean = np.mean(y) # 확률값, 꼭 평균값일 필요는 없는데 손실을 최소화 해줘서 보통 사용한다.\n",
    "        self.initial_log_odds = self.log_odds(y_mean) # 확률값 -> 로그오즈\n",
    "\n",
    "        y_pred_log_odds = np.full_like(y, self.initial_log_odds, dtype = np.float64)\n",
    "        #print('y_pred_log_odds', y_pred_log_odds)\n",
    "\n",
    "        # 반복적으로 약학습기(회귀예측기)를 순차적으로 학습 : 이전 오차를 학습\n",
    "        for _ in range(self.n_estimators):\n",
    "            # 오차를 계산하기 위하여\n",
    "            # 로그오즈값(z) -> 확률값(p) 변환 : 왜 ? 오차를 구하기 위해서\n",
    "            y_pred_proba = self.sigmoid(y_pred_log_odds)\n",
    "                                # 확률값변환 여기서\n",
    "\n",
    "            # 잔차(오차) 계산\n",
    "            residuals = y - y_pred_proba\n",
    "\n",
    "            # 잔차에 대한 학습\n",
    "            dt_reg = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            dt_reg.fit(X, residuals) # 이전 오차에 대한 것을 학습하기 때문에 -> 그러면 어떻게 되냐? 오차를 보고 얼마나 보정을 해야하는 지 알 수 있음\n",
    "\n",
    "\n",
    "            # 현재 예측값으로 로그오즈값 갱신\n",
    "            y_pred = dt_reg.predict(X) # 보정량 예측한 값\n",
    "            y_pred_log_odds -= self.lr * y_pred # 보정량을 조금씩 쪼개서 적용시킨다.\n",
    "                                # 잔차에 대해서 학습을 했으면 얼마나 보정해야할지 보정량이 나올 것이고\n",
    "                                # 보정량을 몇 퍼센트 고정할 지 조금씩\n",
    "            # 약학습기 관리\n",
    "            self.trees.append(dt_reg) # 잔차가 계속 줄어드는 방식으로 반복된다.\n",
    "\n",
    "\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"모델 예측을 위한 확률값 계산 함수\"\"\"\n",
    "\n",
    "        # 초기 로그오즈 설정\n",
    "        y_pred_log_odds = np.full((X.shape[0],), self.initial_log_odds,\n",
    "                                  dtype = np.float64)\n",
    "\n",
    "        # 학습된 약학습기의 예측 결과\n",
    "        for tree in self.trees: # 잔차를 학습한 회귀 트리 반복\n",
    "            y_pred_log_odds += self.lr * tree.predict(X) # 이미 잔차 방향으로 정해져있기 때문에 +를 해준다. (!= 위에서 fit 할 때 (오차 반대방향으로 간다. 손실을 줄이기 위해서))\n",
    "\n",
    "        # 로그오즈값을 확률값으로 변환해서 반환\n",
    "        return self.sigmoid(y_pred_log_odds) # 양성클래스의 확률값만 반환\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"모델 예측 함수\"\"\"\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "sgb_clf = SimpleGradientBoostingClassifier()\n",
    "sgb_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 평가\n",
    "y_pred = sgb_clf.predict(X_train)\n",
    "print(\"Train Accuracy :\", accuracy_score(y_train, y_pred))\n",
    "\n",
    "\n",
    "y_pred = sgb_clf.predict(X_test)\n",
    "print(\"Test Accuracy :\", accuracy_score(y_test, y_pred))"
   ],
   "id": "2c6eb05951b9458",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.9736263736263736\n",
      "Test Accuracy : 0.9385964912280702\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 02. GradientBoostingClassifier",
   "id": "9c405160cd224406"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T04:06:10.931112Z",
     "start_time": "2025-12-19T04:06:10.207866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 이진 분류 : 유방암 진단\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(random_state=42, max_depth=3)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gb_clf.predict(X_train)\n",
    "print(\"Train Accuracy :\", accuracy_score(y_train, y_pred))\n",
    "\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "print(\"Test Accuracy :\", accuracy_score(y_test, y_pred))"
   ],
   "id": "282cf3485ed8e8e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 1.0\n",
      "Test Accuracy : 0.956140350877193\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T04:06:10.988199Z",
     "start_time": "2025-12-19T04:06:10.972084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 다중분류\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "\n",
    "wine = load_wine()\n",
    "X, y = wine.data, wine.target\n",
    "\n",
    "# 타겟 클래스 0 1 2\n",
    "print(np.unique(y, return_counts=True))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ],
   "id": "c1795c0709e854bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1, 2]), array([59, 71, 48]))\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T04:06:11.539751Z",
     "start_time": "2025-12-19T04:06:11.048555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 모델 학습\n",
    "gb_clf = GradientBoostingClassifier(random_state=42, max_depth=3)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = gb_clf.predict(X_train)\n",
    "print(\"Train Accuracy :\", accuracy_score(y_train, y_pred))\n",
    "y_pred = gb_clf.predict(X_test)\n",
    "print(\"Test Accuracy :\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "# classification_report 클래스별 예측성능\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "id": "cb534ecd227d0412",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 1.0\n",
      "Test Accuracy : 0.9074074074074074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.93        19\n",
      "           1       0.90      0.90      0.90        21\n",
      "           2       1.00      0.79      0.88        14\n",
      "\n",
      "    accuracy                           0.91        54\n",
      "   macro avg       0.92      0.90      0.90        54\n",
      "weighted avg       0.91      0.91      0.91        54\n",
      "\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-19T04:06:11.586319Z",
     "start_time": "2025-12-19T04:06:11.583404Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a87755cf64ce7498",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
